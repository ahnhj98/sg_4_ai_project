# -*- coding: utf-8 -*-
"""model1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IwjGqzyojCOrnazH4FUviDLScNPBn5_Y
"""

import yfinance as yf
import pandas as pd
from datetime import datetime, timedelta
from fredapi import Fred

# Define the list of tickers and their corresponding yfinance symbols or FRED series IDs
tickers = {
    "S&P 500": "^GSPC",
    "Dow Jones Industrial Average": "^DJI",
    "NASDAQ Composite Index": "^IXIC",
    "Russell 2000 Index": "^RUT",
    "MSCI World Index": "URTH",
    "FTSE 100": "^FTSE",
    "Nikkei 225": "^N225",
    "Hang Seng Index": "^HSI",
    "Shanghai Composite Index": "000001.SS",
    "Euro Stoxx 50": "^STOXX50E",
    "DAX Index": "^GDAXI",
    "CAC 40": "^FCHI",
    "S&P/ASX 200": "^AXJO",
    "BSE Sensex": "^BSESN",
    "Nifty 50": "^NSEI",
    "US 10Y Treasury Yield": "DGS10",
    "US 2Y Treasury Yield": "DGS2",
    "Federal Funds Rate": "DFF",
    "Dollar Index": "DTWEXBGS",
    "EUR/USD Exchange Rate": "DEXUSEU",
    "USD/JPY Exchange Rate": "DEXJPUS",
    "GBP/USD Exchange Rate": "DEXUSUK",
    "USD/CNY Exchange Rate": "DEXCHUS"
}

# Define the start and end dates
start_date = "2019-01-01"
end_date = "2023-12-31"

# Adjust the start date to ensure the interpolation window
start_date_adjusted = (datetime.strptime(start_date, '%Y-%m-%d') - timedelta(days=7)).strftime('%Y-%m-%d')

fred = Fred(api_key='ec0293b2fb9d336968dfd854c381389a')

# Function to get data from yfinance
def get_yfinance_data(ticker, start, end):
    data = yf.download(ticker, start=start, end=end)
    data = data[['Close']]
    return data

# Function to get data from FRED
def get_fred_data(ticker, start, end):
    data = fred.get_series(ticker, start, end)
    return pd.DataFrame(data, columns=[ticker])

# Initialize an empty dictionary to hold dataframes
data_frames = {}

# Fetch data for each ticker
for name, ticker in tickers.items():
    try:
        if ticker:
            if ticker.startswith('D') or ticker in ['USD3MTD156N', 'DFF', 'CPIAUCSL', 'PPIACO', 'UNRATE', 'A191RL1Q225SBEA']:
                data_frames[name] = get_fred_data(ticker, start_date, end_date)
            else:
                data_frames[name] = get_yfinance_data(ticker, start_date, end_date)
        else:
            print(f"No valid ticker for {name}")
    except Exception as e:
        print(f"Error fetching data for {name} ({ticker}): {e}")

# Function to calculate 7-day change, volatility, and store interpolated values
def calculate_change_and_volatility(df):
    results = []
    df_interpolated = df.interpolate(method='linear')

    for i in range(len(df_interpolated) - 7):
        window = df_interpolated.iloc[i:i+8].copy()

        start_value = window.iloc[0]
        end_value = window.iloc[-1]
        change = end_value - start_value
        volatility = window.max() - window.min()

        # Determine if the value was interpolated or original
        original_value = df.iloc[i].values[0]
        interpolated_value = df_interpolated.iloc[i].values[0]
        value = interpolated_value if pd.isnull(original_value) else original_value

        results.append({
            'date': df.index[i],
            'value': value,
            '7_day_change': change.values[0],
            '7_day_volatility': volatility.values[0]
        })

    return pd.DataFrame(results)

# Create a dictionary to hold the results
change_volatility_frames = {}

# Calculate change and volatility for each dataframe
for name, df in data_frames.items():
    print(f"Processing {name}")
    change_volatility_frames[name] = calculate_change_and_volatility(df)

# Example of accessing the data
for name, df in change_volatility_frames.items():
    print(f"{name} change and volatility data:")
    print(df.head())

# Save each dataframe to a separate CSV file (optional)
for name, df in change_volatility_frames.items():
    safe_name = name.replace(' ', '_').replace('&', 'and').replace('/', '_')
    try:
        df.to_csv(f"/Users/stancho/Documents/SogangUniversity/2024spring/Capstone Design & Start-up Artificial Intelligence/model 학습/거시경제/{safe_name}_change_volatility.csv", index=False)
    except Exception as e:
        print(f"Error saving {name} data: {e}")

"""### Step 2. ETF 가격 예측"""

import os

# ETF 목록
etf_list = [
    "XLK", "VGT", "XLF", "VFH", "XLV", "VHT", "XLE", "VDE",
    "XLI", "VIS", "XLY", "VCR", "XLP", "VDC", "XLC", "VOX",
    "XLU", "VPU", "XLRE", "VNQ", "BND", "AGG", "LQD", "HYG",
    "IEF", "GLD", "IAU", "SLV", "USO", "DBO", "VNQ", "REET",
    "IYR", "UUP", "FXE", "FXY", "FXB", "FXC"
]

# 데이터 저장 경로
save_path = "/Users/stancho/Documents/SogangUniversity/2024spring/Capstone Design & Start-up Artificial Intelligence/model 학습/sector"

# 폴더가 존재하지 않으면 생성
if not os.path.exists(save_path):
    os.makedirs(save_path)

# 시작일과 종료일 설정
start_date = "2013-01-01"
end_date = "2023-12-31"

# 각 ETF의 데이터를 다운로드하고 저장
for etf in etf_list:
    data = yf.download(etf, start=start_date, end=end_date)
    df = data[['Close']].reset_index()
    df.columns = ['Date', 'Close']  # 칼럼 이름 재설정

    file_path = os.path.join(save_path, f"{etf}_price_data.csv")
    df.to_csv(file_path, index=False)
    print(f"{etf} 데이터 저장 완료: {file_path}")

save_path = "/Users/stancho/Documents/SogangUniversity/2024spring/Capstone Design & Start-up Artificial Intelligence/model 학습/sector"

# ETF 목록
etf_list = [
    "XLK", "VGT", "XLF", "VFH", "XLV", "VHT", "XLE", "VDE",
    "XLI", "VIS", "XLY", "VCR", "XLP", "VDC", "XLC", "VOX",
    "XLU", "VPU", "XLRE", "VNQ", "BND", "AGG", "LQD", "HYG",
    "IEF", "GLD", "IAU", "SLV", "USO", "DBO", "VNQ", "REET",
    "IYR", "UUP", "FXE", "FXY", "FXB", "FXC"
]

for etf in etf_list:
    file_path = os.path.join(save_path, f"{etf}_price_data.csv")
    df = pd.read_csv(file_path)

    # 'Date' 컬럼을 datetime 형식으로 변환
    df['Date'] = pd.to_datetime(df['Date'])

    # 7일 후의 날짜 계산
    df['Date_After_7_Days'] = df['Date'] + pd.Timedelta(days=7)

    # 7일 후의 가격을 가져오기 위해 merge 수행
    df_merged = pd.merge(df, df[['Date', 'Close']], left_on='Date_After_7_Days', right_on='Date', suffixes=('', '_After_7_Days'))

    # 7일 후 가격의 비율 계산
    df_merged['7_Day_Percent_Change'] = ((df_merged['Close_After_7_Days'] / df_merged['Close']) - 1) * 100

    # 필요한 컬럼만 유지
    df_final = df_merged[['Date', 'Close', '7_Day_Percent_Change']]

    # 다시 저장
    df_final.to_csv(file_path, index=False)
    print(f"{etf} 데이터 업데이트 완료: {file_path}")

all_data = {}

# Load the macroeconomic factors data
macro_factors_path = "/Users/stancho/Documents/SogangUniversity/2024spring/Capstone Design & Start-up Artificial Intelligence/model 학습/거시경제/"
for name in tickers.keys():
    safe_name = name.replace(' ', '_').replace('&', 'and').replace('/', '_')
    file_path = os.path.join(macro_factors_path, f"{safe_name}_change_volatility.csv")
    try:
        df = pd.read_csv(file_path)
        df['Date'] = pd.to_datetime(df['date'])
        df = df[['Date', '7_day_change', '7_day_volatility']]
        all_data[safe_name] = df
    except Exception as e:
        print(f"Error loading macroeconomic factor data for {name}: {e}")

# Load the ETF data
sector_path = "/Users/stancho/Documents/SogangUniversity/2024spring/Capstone Design & Start-up Artificial Intelligence/model 학습/sector/"
for etf in etf_list:
    file_path = os.path.join(sector_path, f"{etf}_price_data.csv")
    try:
        df = pd.read_csv(file_path)
        df['Date'] = pd.to_datetime(df['Date'])
        df = df[['Date', '7_Day_Percent_Change']]
        all_data[etf] = df
    except Exception as e:
        print(f"Error loading ETF data for {etf}: {e}")

# Merge all dataframes on the 'Date' column
merged_df = pd.DataFrame({'Date': pd.date_range(start=start_date, end=end_date)})

for name, df in all_data.items():
    merged_df = pd.merge(merged_df, df, on='Date', how='left', suffixes=('', f'_{name}'))

# Drop rows with any missing values
merged_df = merged_df.dropna()

# Display the first few rows of the merged dataframe
print(merged_df.head())

# Save the merged dataframe to a CSV file
output_path = "/Users/stancho/Documents/SogangUniversity/2024spring/Capstone Design & Start-up Artificial Intelligence/model 학습/merged_data.csv"
try:
    merged_df.to_csv(output_path, index=False)
    print(f"Merged data saved to {output_path}")
    print(merged_df.head())
except Exception as e:
    print(f"Error saving merged data: {e}")

merged_df

import pandas as pd
import requests
import zipfile
import os
import glob
from datetime import datetime, timedelta

# Function to download and extract GDELT data
def download_gdelt_data(date):
    url = f"http://data.gdeltproject.org/events/{date}.export.CSV.zip"
    local_zip_file = f"./gdelt/{date}.zip"
    response = requests.get(url, stream=True)
    with open(local_zip_file, 'wb') as f:
        for chunk in response.iter_content(chunk_size=128):
            f.write(chunk)
    with zipfile.ZipFile(local_zip_file, 'r') as zip_ref:
        zip_ref.extractall(f"./gdelt/{date}")
    print(f"Downloaded and extracted data for date: {date}")

# Create a list of dates for processing (last 10 years)
end_date = datetime.today()
start_date = end_date - timedelta(days=365 * 10)
dates = pd.date_range(start=start_date, end=end_date, freq='D').strftime('%Y%m%d').tolist()

# Create directories if they do not exist
if not os.path.exists('./gdelt'):
    os.makedirs('./gdelt')

all_data = []
for date in dates:
    try:
        download_gdelt_data(date)
        files = glob.glob(f"./gdelt/{date}/*.CSV")
        for file_path in files:
            print(f"Processing file: {file_path}")
            try:
                df = pd.read_csv(file_path, sep='\t', header=None, dtype=str, usecols=[1, 4, 26])  # Adjust columns as necessary
                df.columns = ['Date', 'URL', 'Title']

                # Select top 30 rows based on some importance metric if available
                # For now, just select the first 30 rows
                df = df.head(30)

                # Keep only 'Date' and 'Title' columns
                df = df[['Date', 'Title']]

                all_data.append(df)
            except Exception as e:
                print(f"Error processing file {file_path}: {e}")
    except Exception as e:
        print(f"Error processing date {date}: {e}")

# Concatenate all data into one DataFrame
if all_data:
    final_df = pd.concat(all_data, ignore_index=True)
    print(f"Sample of the data collected:\n{final_df.head()}")

    # Save the results to a CSV file
    save_path = "./top_30_news_titles_10_years.csv"
    final_df.to_csv(save_path, index=False)
    print(f"Results saved to {save_path}")
else:
    print("No data was processed.")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
import joblib

# Load the merged dataset
merged_data_path = "/Users/stancho/Documents/SogangUniversity/2024spring/Capstone Design & Start-up Artificial Intelligence/model 학습/merged_data.csv"
merged_df = pd.read_csv(merged_data_path)

# Define the input and output columns
input_columns = [col for col in merged_df.columns if '7_day_change' in col or '7_day_volatility' in col]
output_columns = [col for col in merged_df.columns if '7_Day_Percent_Change' in col]

# Drop the date column
merged_df = merged_df.drop(columns=['Date'])

# Split the data into input (X) and output (y)
X = merged_df[input_columns]
y = merged_df[output_columns]

# Standardize the input features
scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X)

# Standardize the output features
scaler_y = StandardScaler()
y_scaled = scaler_y.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)

# Define the optimizer
optimizer = Adam(learning_rate=0.0001)

# Define the model architecture with more hidden layers and dropout
model = Sequential([
    Dense(512, input_shape=(X_train.shape[1],), activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    Dropout(0.5),
    Dense(256, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    Dropout(0.5),
    Dense(128, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    Dropout(0.5),
    Dense(64, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    Dropout(0.5),
    Dense(32, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    Dropout(0.5),
    Dense(y_train.shape[1])  # Output layer with the number of outputs matching y_train
])

# Compile the model
model.compile(optimizer=optimizer, loss='mean_squared_error')

# Early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model
history = model.fit(X_train, y_train, epochs=10000, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=1)

# Plot MSE for each epoch
train_mse = history.history['loss']
val_mse = history.history['val_loss']
epochs = range(1, len(train_mse) + 1)

plt.plot(epochs, train_mse, 'b', label='Training MSE')
plt.plot(epochs, val_mse, 'r', label='Validation MSE')
plt.title('Training and Validation MSE')
plt.xlabel('Epochs')
plt.ylabel('MSE')
plt.legend()
plt.show()

# Evaluate the model on the test set
test_predictions = model.predict(X_test)
test_predictions = scaler_y.inverse_transform(test_predictions)  # Rescale predictions
y_test_rescaled = scaler_y.inverse_transform(y_test)  # Rescale true values

mse_test = mean_squared_error(y_test_rescaled, test_predictions)
rmse_test = np.sqrt(mse_test)

print(f'Test RMSE: {rmse_test}')

# Save the model
model_path = "/Users/stancho/Documents/SogangUniversity/2024spring/Capstone Design & Start-up Artificial Intelligence/model 학습/deep_learning_model.h5"
model.save(model_path)
print(f"Model saved to {model_path}")

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import load_model
import matplotlib.pyplot as plt

# Load the dataset for the new data (yesterday to 7 days ago)
new_data_path = "/Users/stancho/Documents/SogangUniversity/2024spring/Capstone Design & Start-up Artificial Intelligence/model 학습/merged_data.csv"
new_data_df = pd.read_csv(new_data_path)

# Get the data from yesterday to 7 days ago
recent_data = new_data_df[-20:]

# Define the input and output columns
input_columns = [col for col in recent_data.columns if '7_day_change' in col or '7_day_volatility' in col]
output_columns = [col for col in recent_data.columns if '7_Day_Percent_Change' in col]

# Drop the date column
recent_data = recent_data.drop(columns=['Date'])

# Split the data into input (X) and output (y)
X_recent = recent_data[input_columns]
y_actual = recent_data[output_columns]

# Standardize the input features using the same scaler used during training
scaler_X = StandardScaler()
X_recent_scaled = scaler_X.fit_transform(X_recent)  # Use fit_transform instead of transform for new scaling

# Load the trained model
model_path = "/Users/stancho/Documents/SogangUniversity/2024spring/Capstone Design & Start-up Artificial Intelligence/model 학습/deep_learning_model.h5"
model = load_model(model_path)

# Make predictions on the recent data
predictions_scaled = model.predict(X_recent_scaled)

# Rescale predictions back to the original scale
scaler_y = StandardScaler()
scaler_y.fit(y_actual)  # Fit the scaler on the actual recent data to match scaling
predictions = scaler_y.inverse_transform(predictions_scaled)

# Plot the comparison of predicted vs actual values for each ETF
for i, column in enumerate(output_columns):
    plt.figure(figsize=(10, 5))
    index = np.arange(len(predictions))
    bar_width = 0.35
    plt.bar(index, predictions[:, i], bar_width, label='Predicted')
    plt.bar(index + bar_width, y_actual.iloc[:, i], bar_width, label='Actual')
    plt.xlabel('Index')
    plt.ylabel('Price After 7 Days')
    plt.title(f'Predicted vs Actual Price After 7 Days for {column}')
    plt.xticks(index + bar_width / 2, recent_data.index)
    plt.legend()
    plt.show()

import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta

# Function to fetch top news headlines from Yahoo Finance for a given date
def fetch_yahoo_finance_news(date):
    url = f"https://finance.yahoo.com/calendar/earnings?day={date}"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    # Adjust the selector based on the actual HTML structure of Yahoo Finance
    headlines = soup.select('h3 a')  # Example selector, adjust as necessary
    news_titles = [headline.get_text().strip() for headline in headlines[:30]]  # Top 30 news titles
    return news_titles

# Create a list of dates for the last 10 years
end_date = datetime.today()
start_date = end_date - timedelta(days=10)
dates = pd.date_range(start=start_date, end=end_date, freq='D').strftime('%Y-%m-%d').tolist()

all_data = []
for date in dates:
    try:
        print(f"Fetching top news for date: {date}")
        news_titles = fetch_yahoo_finance_news(date)
        for title in news_titles:
            all_data.append({'Date': date, 'Title': title})
    except Exception as e:
        print(f"Error fetching news for date {date}: {e}")

# Create a DataFrame from the collected data
if all_data:
    df = pd.DataFrame(all_data)
    print(f"Sample of the data collected:\n{df.head()}")

    # Save the results to a CSV file
    save_path = "./yahoo_finance_top_news_titles_10_years.csv"
    df.to_csv(save_path, index=False)
    print(f"Results saved to {save_path}")
else:
    print("No data was processed.")

import requests
import pandas as pd
from datetime import datetime, timedelta

API_KEY = '4aeec77f6d1c49f1890a92eca159d4b5'
BASE_URL = 'https://newsapi.org/v2/everything'

def fetch_articles(api_key, start_date, end_date):
    all_articles = []
    url = BASE_URL
    params = {
        'domains': 'wsj.com',
        'apiKey': api_key,
        'from': start_date,
        'to': end_date,
        'pageSize': 100,
        'page': 1,
        'sortBy': 'publishedAt'
    }

    while True:
        response = requests.get(url, params=params)
        data = response.json()

        if data.get('status') != 'ok':
            print(f"Error fetching articles: {data.get('message')}")
            break

        articles = data.get('articles', [])
        all_articles.extend(articles)

        if len(articles) < 100:
            break

        params['page'] += 1

        if params['page'] > 5:  # To avoid too many requests in one go
            break

    return all_articles

def collect_data(api_key, years=10):
    end_date = datetime.utcnow()
    start_date = end_date - timedelta(days=365 * years)
    date_ranges = pd.date_range(start=start_date, end=end_date, freq='M')

    all_data = []

    for i in range(len(date_ranges) - 1):
        start = date_ranges[i].strftime('%Y-%m-%d')
        end = date_ranges[i + 1].strftime('%Y-%m-%d')

        print(f"Fetching articles from {start} to {end}")
        articles = fetch_articles(api_key, start, end)

        for article in articles:
            published_at = article.get('publishedAt')
            title = article.get('title')
            if published_at and title:
                date = published_at.split('T')[0]
                all_data.append({'Date': date, 'Title': title})

    return all_data

def main():
    all_data = collect_data(API_KEY, years=10)

    if all_data:
        df = pd.DataFrame(all_data)
        print(f"Sample of the data collected:\n{df.head()}")
        save_path = "./wsj_top_news_titles_10_years.csv"
        df.to_csv(save_path, index=False)
        print(f"Results saved to {save_path}")
    else:
        print("No data was processed.")

if __name__ == "__main__":
    main()

